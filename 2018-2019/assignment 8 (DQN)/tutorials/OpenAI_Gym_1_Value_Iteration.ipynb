{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym (Value iteration algorithm)\n",
    "https://www.kaggle.com/charel/learn-by-example-reinforcement-learning-with-gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Taxi-v2 env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "env.env.s = 26\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Actions__:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pick up passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "__Rendering__:\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- |: wall\n",
    "- other letters: locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration algorithm\n",
    "* The value iteration algorithm is centered around the game states;\n",
    "* It has to know all environment states/transitions upfront (__model-based__ method);\n",
    "* The core idea of the algorithm is to calculate the value (expected long term maximum results) of each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "\n",
    "$V^{\\ast}(s) = \\max \\limits_{a} \\{ R(s, a) + \\gamma \\sum\\limits_{s'} {P(s'|s, a) V^{\\ast}(s')} \\}$  \n",
    "\n",
    "- R(s, a): Reward of action a in state s.\n",
    "- P(s'|s, a): Probability of going to state s' given action a in state s. The Taxi game actions are deterministic =>  P(...) = 1 always.\n",
    "- $\\gamma$: Discount factor between 0 and 1. Higher $\\gamma$ means a higher focus on long term rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ACTIONS: 6\n",
      "NUM_STATES: 500\n"
     ]
    }
   ],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "print(f'NUM_ACTIONS: {NUM_ACTIONS}')\n",
    "print(f'NUM_STATES: {NUM_STATES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.zeros(NUM_STATES)\n",
    "pi = np.zeros(NUM_STATES, dtype=int)\n",
    "gamma = 0.9\n",
    "significant_improvement = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(env, s):\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    \n",
    "    for a in range(NUM_ACTIONS):\n",
    "        env.env.s = s\n",
    "        s_next, reward, _, _ = env.step(a)\n",
    "        value = reward + gamma * v[s_next]\n",
    "        if best_value < value:\n",
    "            best_value = value\n",
    "            best_a = a\n",
    "    \n",
    "    env.env.s = s\n",
    "    return best_a\n",
    "\n",
    "def value_iteration(env):\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        max_change = 0\n",
    "        env.reset()\n",
    "        \n",
    "        for s in range(NUM_STATES):\n",
    "            v_old = v[s]\n",
    "            \n",
    "            # choose best_action for state s\n",
    "            best_a = best_action(env, s)\n",
    "            \n",
    "            # step with best_action\n",
    "            env.env.s = s\n",
    "            s_next, reward, _, _ = env.step(best_a)\n",
    "            v[s] = reward + gamma * v[s_next]\n",
    "            \n",
    "            # save best_a for state s\n",
    "            pi[s] = best_a\n",
    "            \n",
    "            # update max_change\n",
    "            max_change = max(max_change, abs(v_old - v[s]))\n",
    "            \n",
    "        iteration += 1\n",
    "        if max_change < significant_improvement:\n",
    "            print(f'{iteration} iterations done')\n",
    "            break\n",
    "            \n",
    "def solve_game(env, pi, s=404):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    env.reset()\n",
    "    env.env.s = s\n",
    "    env.render()\n",
    "    \n",
    "    while not done:\n",
    "        a = pi[s]\n",
    "        s, r, done, _ = env.step(a)\n",
    "        env.render()\n",
    "        print(f'reward: {r}')\n",
    "        total_reward += r\n",
    "        \n",
    "    print(f'total reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 iterations done\n"
     ]
    }
   ],
   "source": [
    "value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "reward: -1\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "reward: 20\n",
      "total reward: 5\n"
     ]
    }
   ],
   "source": [
    "solve_game(env, pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
