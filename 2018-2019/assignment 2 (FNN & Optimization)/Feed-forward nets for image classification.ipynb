{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmaHAc_KHG_5"
   },
   "source": [
    "# ANN - Lab 2: Feed-forward nets for image classification\n",
    "\n",
    "Tudor Berariu  \n",
    "Skeleton code from https://github.com/tudor-berariu/ann2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgNIWhzoHOIz"
   },
   "source": [
    "## 1. The MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQiqJyO7E7Ek"
   },
   "outputs": [],
   "source": [
    "#!pip install mnist\n",
    "\n",
    "import mnist\n",
    "train_imgs = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_imgs = mnist.test_images()\n",
    "test_labels  = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization\n",
    "\n",
    "Rescale input values to have zero mean and standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std  = train_imgs.mean(), train_imgs.std()\n",
    "train_imgs = (train_imgs - mean) / std\n",
    "test_imgs = (test_imgs - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "colab_type": "code",
    "id": "qeftJ_CpE7Eu",
    "outputId": "c7a9a956-0114-4975-90a0-8af5b2007c2b"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "colab_type": "code",
    "id": "qeftJ_CpE7Eu",
    "outputId": "c7a9a956-0114-4975-90a0-8af5b2007c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [4 2 6 0 4 4 3 0 0 5 7 1 7 1 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idxs = np.random.randint(0, len(train_imgs), 15)\n",
    "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
    "plt.imshow(imgs)\n",
    "print(\"Labels:\", train_labels[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgERUA07IuSr"
   },
   "source": [
    "## 2. Building feed forward-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "Each layer will have three methods:\n",
    " - `forward` computes and returns ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf w}^{(l)}\\right)$\n",
    " - `backward` gets $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, and stores $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$ internally, and returns $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$\n",
    " - `update` modifies parameters ${\\bf w}^{(l)}$ using stored $\\frac{\\partial{\\cal L}}{\\partial{\\bf w}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass  # If a layer has no parameters, then this function does nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feed-forward netowork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
    "        self._inputs = []\n",
    "        for layer in self.layers:\n",
    "            if train:\n",
    "                self._inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
    "        # TODO <0> : Compute the backward phase\n",
    "        for idx in reversed(range(len(self._inputs))):\n",
    "            x = self._inputs[idx]\n",
    "            layer = self.layers[idx]\n",
    "            dy = layer.backward(x, dy)\n",
    "        del self._inputs\n",
    "        return dy\n",
    "    \n",
    "    def update(self, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            layer.update(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S47ZsyKdE7FF"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, insize: int, outsize: int) -> None:\n",
    "        bound = np.sqrt(6. / insize)\n",
    "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
    "        self.bias = np.zeros((outsize,))\n",
    "        \n",
    "        self.dweight = np.zeros_like(self.weight)\n",
    "        self.dbias = np.zeros_like(self.bias)\n",
    "        \n",
    "        self.weight_v = np.zeros_like(self.weight)\n",
    "        self.dweight_cache = np.zeros_like(self.dweight)\n",
    "        self.bias_v = np.zeros_like(self.bias)\n",
    "        self.dbias_cache = np.zeros_like(self.dbias)\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <1> : compute the output of a linear layer\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <2> : compute dweight, dbias and  return dx\n",
    "        self.dweight = np.dot(x.T, dy)\n",
    "        self.dbias = np.sum(dy, axis=0)\n",
    "        self.dx = np.dot(dy, self.weight.T)\n",
    "        return self.dx\n",
    "    \n",
    "    def update(self, mode='SGD', lr=0.001, mu=.9, beta1=.9, beta2=.999):\n",
    "        if mode == 'SGD':\n",
    "            self.weight -= lr * self.dweight\n",
    "            self.bias -= lr * self.dbias\n",
    "        elif mode == 'Nesterov':\n",
    "            # TODO <9> : compute the nesterov update (for Lab 2)\n",
    "            weight_v_prev = self.weight_v.copy()\n",
    "            bias_v_prev = self.bias_v.copy()\n",
    "            \n",
    "            self.weight_v = mu * self.weight_v - lr * self.dweight\n",
    "            self.bias_v = mu * self.bias_v - lr * self.dbias\n",
    "            \n",
    "            self.weight += -mu * weight_v_prev + (1 + mu) * self.weight_v\n",
    "            self.bias += -mu * bias_v_prev + (1 + mu) * self.bias_v\n",
    "            \n",
    "            del weight_v_prev\n",
    "            del bias_v_prev\n",
    "        elif mode == 'Adam':\n",
    "            # TODO <10> : compute the Adam update  (for Lab 2)\n",
    "            self.weight_v = beta1 * self.weight_v + (1 - beta1) * self.dweight\n",
    "            self.dweight_cache = beta2 * self.dweight_cache + (1 - beta2) * self.dweight ** 2\n",
    "            \n",
    "            self.bias_v = beta1 * self.bias_v + (1 - beta1) * self.dbias\n",
    "            self.dbias_cache = beta2 * self.dbias_cache + (1 - beta2) * self.dbias ** 2\n",
    "            \n",
    "            self.weight -= lr * self.weight_v / (1e-8 + np.sqrt(self.dweight_cache))\n",
    "            self.bias -= lr * self.bias_v / (1e-8 + np.sqrt(self.dbias_cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rectified Linear Unit\n",
    "$$y = \\max\\left(x, 0\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOR1DJiwE7FJ"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <3> : Compute the output of a rectified linear unit\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <4> : Compute the gradient w.r.t. x\n",
    "        return (x > 0) * dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NrWBTmbI9gW"
   },
   "source": [
    "## 3. The loss function\n",
    "\n",
    "   The negative log likelihood combines a softmax activation, and a cross-entropy cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDXiDEu8E7FW"
   },
   "outputs": [],
   "source": [
    "class NegativeLogLikelihood:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
    "        # TODO <5> : Compute the negative log likelihood\n",
    "        self._N = y.shape[0]\n",
    "        shifted_logits = y - np.max(y, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "        self._log_probs = shifted_logits - np.log(Z)\n",
    "        loss = -np.sum(self._log_probs[np.arange(self._N), t]) / self._N\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        # TODO <6> : Compute dl/dy\n",
    "        dy = np.exp(self._log_probs)\n",
    "        dy[np.arange(self._N), t] -= 1\n",
    "        dy /= self._N\n",
    "        return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uz9qM5eHJLNw"
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nYfVCBSE7Fe"
   },
   "outputs": [],
   "source": [
    "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
    "    # TODO <7> : Compute accuracy\n",
    "    return np.sum(np.argmax(y, axis=1) == t) / np.float64(t.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIhtzd2gJQF2"
   },
   "source": [
    "## 4. Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "HTbmZv3YE7Fs",
    "outputId": "d6bb5b23-201b-4c2f-cf88-37f90bb5f6a6"
   },
   "outputs": [],
   "source": [
    "def train_a_neural_net(optimize_args = {'mode': 'SGD', 'lr': .001}):\n",
    "    BATCH_SIZE = 128\n",
    "    HIDDEN_UNITS = 200\n",
    "    EPOCHS_NO = 50\n",
    "\n",
    "    net = FeedForwardNetwork([Linear(784, HIDDEN_UNITS),\n",
    "                              ReLU(),\n",
    "                              Linear(HIDDEN_UNITS, 10)])\n",
    "    nll = NegativeLogLikelihood()\n",
    "\n",
    "    for epoch in range(EPOCHS_NO):\n",
    "        for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
    "            # 1. Prepare next batch\n",
    "            x = train_imgs[idx:idx + BATCH_SIZE,:,:].reshape(-1, 784)\n",
    "            t = train_labels[idx:idx + BATCH_SIZE]\n",
    "\n",
    "            # forward\n",
    "            y = net.forward(x, train=True)\n",
    "\n",
    "            # 2. Compute gradient\n",
    "            # TODO <8> : Compute gradient\n",
    "            loss = nll.forward(y, t)\n",
    "            dy = nll.backward(y, t)\n",
    "            net.backward(dy)\n",
    "\n",
    "            # 3. Update network parameters\n",
    "            net.update(**optimize_args)\n",
    "\n",
    "            print(f'\\rEpoch {epoch + 1:02d} '\n",
    "                  f'| Batch {b_no:03d} '\n",
    "                  f'| Train NLL: {loss:3.5f} '\n",
    "                  f'| Train Accuracy: {accuracy(y, t):3.2f} ', end='')\n",
    "\n",
    "        y = net.forward(test_imgs.reshape(-1, 784), train=False)\n",
    "        test_nll = nll.forward(y, test_labels)\n",
    "        print(f' | Test NLL: {test_nll:3.5f} '\n",
    "              f' | Test Accuracy: {accuracy(y, test_labels):3.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Batch 468 | Train NLL: 1.03884 | Train Accuracy: 0.72  | Test NLL: 0.89364  | Test Accuracy: 0.74\n",
      "Epoch 02 | Batch 468 | Train NLL: 0.73349 | Train Accuracy: 0.81  | Test NLL: 0.62246  | Test Accuracy: 0.82\n",
      "Epoch 03 | Batch 468 | Train NLL: 0.60450 | Train Accuracy: 0.88  | Test NLL: 0.51502  | Test Accuracy: 0.86\n",
      "Epoch 04 | Batch 468 | Train NLL: 0.53322 | Train Accuracy: 0.88  | Test NLL: 0.45650  | Test Accuracy: 0.87\n",
      "Epoch 05 | Batch 468 | Train NLL: 0.48780 | Train Accuracy: 0.89  | Test NLL: 0.41904  | Test Accuracy: 0.88\n",
      "Epoch 06 | Batch 468 | Train NLL: 0.45604 | Train Accuracy: 0.89  | Test NLL: 0.39252  | Test Accuracy: 0.89\n",
      "Epoch 07 | Batch 468 | Train NLL: 0.43235 | Train Accuracy: 0.90  | Test NLL: 0.37246  | Test Accuracy: 0.90\n",
      "Epoch 08 | Batch 468 | Train NLL: 0.41374 | Train Accuracy: 0.90  | Test NLL: 0.35653  | Test Accuracy: 0.90\n",
      "Epoch 09 | Batch 468 | Train NLL: 0.39857 | Train Accuracy: 0.90  | Test NLL: 0.34345  | Test Accuracy: 0.90\n",
      "Epoch 10 | Batch 468 | Train NLL: 0.38598 | Train Accuracy: 0.91  | Test NLL: 0.33238  | Test Accuracy: 0.91\n",
      "Epoch 11 | Batch 468 | Train NLL: 0.37523 | Train Accuracy: 0.91  | Test NLL: 0.32285  | Test Accuracy: 0.91\n",
      "Epoch 12 | Batch 468 | Train NLL: 0.36581 | Train Accuracy: 0.92  | Test NLL: 0.31448  | Test Accuracy: 0.91\n",
      "Epoch 13 | Batch 468 | Train NLL: 0.35742 | Train Accuracy: 0.93  | Test NLL: 0.30702  | Test Accuracy: 0.91\n",
      "Epoch 14 | Batch 468 | Train NLL: 0.35000 | Train Accuracy: 0.94  | Test NLL: 0.30032  | Test Accuracy: 0.92\n",
      "Epoch 15 | Batch 468 | Train NLL: 0.34337 | Train Accuracy: 0.94  | Test NLL: 0.29422  | Test Accuracy: 0.92\n",
      "Epoch 16 | Batch 468 | Train NLL: 0.33747 | Train Accuracy: 0.95  | Test NLL: 0.28864  | Test Accuracy: 0.92\n",
      "Epoch 17 | Batch 468 | Train NLL: 0.33212 | Train Accuracy: 0.95  | Test NLL: 0.28349  | Test Accuracy: 0.92\n",
      "Epoch 18 | Batch 468 | Train NLL: 0.32723 | Train Accuracy: 0.95  | Test NLL: 0.27871  | Test Accuracy: 0.92\n",
      "Epoch 19 | Batch 468 | Train NLL: 0.32268 | Train Accuracy: 0.95  | Test NLL: 0.27423  | Test Accuracy: 0.92\n",
      "Epoch 20 | Batch 468 | Train NLL: 0.31845 | Train Accuracy: 0.95  | Test NLL: 0.27004  | Test Accuracy: 0.92\n",
      "Epoch 21 | Batch 468 | Train NLL: 0.31433 | Train Accuracy: 0.95  | Test NLL: 0.26608  | Test Accuracy: 0.92\n",
      "Epoch 22 | Batch 468 | Train NLL: 0.31043 | Train Accuracy: 0.95  | Test NLL: 0.26234  | Test Accuracy: 0.92\n",
      "Epoch 23 | Batch 468 | Train NLL: 0.30681 | Train Accuracy: 0.96  | Test NLL: 0.25880  | Test Accuracy: 0.93\n",
      "Epoch 24 | Batch 468 | Train NLL: 0.30340 | Train Accuracy: 0.96  | Test NLL: 0.25543  | Test Accuracy: 0.93\n",
      "Epoch 25 | Batch 468 | Train NLL: 0.30010 | Train Accuracy: 0.96  | Test NLL: 0.25222  | Test Accuracy: 0.93\n",
      "Epoch 26 | Batch 468 | Train NLL: 0.29695 | Train Accuracy: 0.96  | Test NLL: 0.24913  | Test Accuracy: 0.93\n",
      "Epoch 27 | Batch 468 | Train NLL: 0.29394 | Train Accuracy: 0.96  | Test NLL: 0.24619  | Test Accuracy: 0.93\n",
      "Epoch 28 | Batch 468 | Train NLL: 0.29109 | Train Accuracy: 0.96  | Test NLL: 0.24334  | Test Accuracy: 0.93\n",
      "Epoch 29 | Batch 468 | Train NLL: 0.28837 | Train Accuracy: 0.96  | Test NLL: 0.24062  | Test Accuracy: 0.93\n",
      "Epoch 30 | Batch 468 | Train NLL: 0.28576 | Train Accuracy: 0.96  | Test NLL: 0.23800  | Test Accuracy: 0.93\n",
      "Epoch 31 | Batch 468 | Train NLL: 0.28327 | Train Accuracy: 0.96  | Test NLL: 0.23546  | Test Accuracy: 0.93\n",
      "Epoch 32 | Batch 468 | Train NLL: 0.28094 | Train Accuracy: 0.96  | Test NLL: 0.23302  | Test Accuracy: 0.93\n",
      "Epoch 33 | Batch 468 | Train NLL: 0.27870 | Train Accuracy: 0.96  | Test NLL: 0.23067  | Test Accuracy: 0.93\n",
      "Epoch 34 | Batch 468 | Train NLL: 0.27652 | Train Accuracy: 0.96  | Test NLL: 0.22839  | Test Accuracy: 0.93\n",
      "Epoch 35 | Batch 468 | Train NLL: 0.27439 | Train Accuracy: 0.96  | Test NLL: 0.22618  | Test Accuracy: 0.93\n",
      "Epoch 36 | Batch 468 | Train NLL: 0.27232 | Train Accuracy: 0.96  | Test NLL: 0.22404  | Test Accuracy: 0.93\n",
      "Epoch 37 | Batch 468 | Train NLL: 0.27034 | Train Accuracy: 0.96  | Test NLL: 0.22197  | Test Accuracy: 0.93\n",
      "Epoch 38 | Batch 468 | Train NLL: 0.26842 | Train Accuracy: 0.96  | Test NLL: 0.21995  | Test Accuracy: 0.94\n",
      "Epoch 39 | Batch 468 | Train NLL: 0.26653 | Train Accuracy: 0.96  | Test NLL: 0.21799  | Test Accuracy: 0.94\n",
      "Epoch 40 | Batch 468 | Train NLL: 0.26474 | Train Accuracy: 0.96  | Test NLL: 0.21609  | Test Accuracy: 0.94\n",
      "Epoch 41 | Batch 468 | Train NLL: 0.26302 | Train Accuracy: 0.96  | Test NLL: 0.21423  | Test Accuracy: 0.94\n",
      "Epoch 42 | Batch 468 | Train NLL: 0.26132 | Train Accuracy: 0.96  | Test NLL: 0.21242  | Test Accuracy: 0.94\n",
      "Epoch 43 | Batch 468 | Train NLL: 0.25964 | Train Accuracy: 0.96  | Test NLL: 0.21066  | Test Accuracy: 0.94\n",
      "Epoch 44 | Batch 468 | Train NLL: 0.25795 | Train Accuracy: 0.96  | Test NLL: 0.20893  | Test Accuracy: 0.94\n",
      "Epoch 45 | Batch 468 | Train NLL: 0.25633 | Train Accuracy: 0.96  | Test NLL: 0.20724  | Test Accuracy: 0.94\n",
      "Epoch 46 | Batch 468 | Train NLL: 0.25475 | Train Accuracy: 0.96  | Test NLL: 0.20560  | Test Accuracy: 0.94\n",
      "Epoch 47 | Batch 468 | Train NLL: 0.25318 | Train Accuracy: 0.96  | Test NLL: 0.20399  | Test Accuracy: 0.94\n",
      "Epoch 48 | Batch 468 | Train NLL: 0.25162 | Train Accuracy: 0.96  | Test NLL: 0.20243  | Test Accuracy: 0.94\n",
      "Epoch 49 | Batch 468 | Train NLL: 0.25008 | Train Accuracy: 0.96  | Test NLL: 0.20089  | Test Accuracy: 0.94\n",
      "Epoch 50 | Batch 468 | Train NLL: 0.24857 | Train Accuracy: 0.96  | Test NLL: 0.19940  | Test Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "train_a_neural_net(optimize_args = {'mode': 'SGD', 'lr': .001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Batch 468 | Train NLL: 0.39991 | Train Accuracy: 0.90  | Test NLL: 0.33342  | Test Accuracy: 0.91\n",
      "Epoch 02 | Batch 468 | Train NLL: 0.31126 | Train Accuracy: 0.95  | Test NLL: 0.26977  | Test Accuracy: 0.92\n",
      "Epoch 03 | Batch 468 | Train NLL: 0.27299 | Train Accuracy: 0.96  | Test NLL: 0.23772  | Test Accuracy: 0.93\n",
      "Epoch 04 | Batch 468 | Train NLL: 0.24892 | Train Accuracy: 0.96  | Test NLL: 0.21581  | Test Accuracy: 0.94\n",
      "Epoch 05 | Batch 468 | Train NLL: 0.23220 | Train Accuracy: 0.97  | Test NLL: 0.19936  | Test Accuracy: 0.94\n",
      "Epoch 06 | Batch 468 | Train NLL: 0.22012 | Train Accuracy: 0.98  | Test NLL: 0.18627  | Test Accuracy: 0.94\n",
      "Epoch 07 | Batch 468 | Train NLL: 0.21086 | Train Accuracy: 0.99  | Test NLL: 0.17555  | Test Accuracy: 0.95\n",
      "Epoch 08 | Batch 468 | Train NLL: 0.20321 | Train Accuracy: 0.99  | Test NLL: 0.16650  | Test Accuracy: 0.95\n",
      "Epoch 09 | Batch 468 | Train NLL: 0.19691 | Train Accuracy: 0.99  | Test NLL: 0.15867  | Test Accuracy: 0.95\n",
      "Epoch 10 | Batch 468 | Train NLL: 0.19166 | Train Accuracy: 0.99  | Test NLL: 0.15183  | Test Accuracy: 0.96\n",
      "Epoch 11 | Batch 468 | Train NLL: 0.18648 | Train Accuracy: 0.99  | Test NLL: 0.14583  | Test Accuracy: 0.96\n",
      "Epoch 12 | Batch 468 | Train NLL: 0.18227 | Train Accuracy: 0.99  | Test NLL: 0.14052  | Test Accuracy: 0.96\n",
      "Epoch 13 | Batch 468 | Train NLL: 0.17869 | Train Accuracy: 0.99  | Test NLL: 0.13573  | Test Accuracy: 0.96\n",
      "Epoch 14 | Batch 468 | Train NLL: 0.17519 | Train Accuracy: 0.99  | Test NLL: 0.13146  | Test Accuracy: 0.96\n",
      "Epoch 15 | Batch 468 | Train NLL: 0.17205 | Train Accuracy: 0.99  | Test NLL: 0.12762  | Test Accuracy: 0.96\n",
      "Epoch 16 | Batch 468 | Train NLL: 0.16941 | Train Accuracy: 0.99  | Test NLL: 0.12412  | Test Accuracy: 0.96\n",
      "Epoch 17 | Batch 468 | Train NLL: 0.16692 | Train Accuracy: 0.99  | Test NLL: 0.12093  | Test Accuracy: 0.96\n",
      "Epoch 18 | Batch 468 | Train NLL: 0.16459 | Train Accuracy: 0.99  | Test NLL: 0.11800  | Test Accuracy: 0.96\n",
      "Epoch 19 | Batch 468 | Train NLL: 0.16245 | Train Accuracy: 0.99  | Test NLL: 0.11531  | Test Accuracy: 0.97\n",
      "Epoch 20 | Batch 468 | Train NLL: 0.16059 | Train Accuracy: 0.99  | Test NLL: 0.11284  | Test Accuracy: 0.97\n",
      "Epoch 21 | Batch 468 | Train NLL: 0.15893 | Train Accuracy: 0.99  | Test NLL: 0.11054  | Test Accuracy: 0.97\n",
      "Epoch 22 | Batch 468 | Train NLL: 0.15739 | Train Accuracy: 0.99  | Test NLL: 0.10843  | Test Accuracy: 0.97\n",
      "Epoch 23 | Batch 468 | Train NLL: 0.15609 | Train Accuracy: 0.99  | Test NLL: 0.10649  | Test Accuracy: 0.97\n",
      "Epoch 24 | Batch 468 | Train NLL: 0.15489 | Train Accuracy: 0.99  | Test NLL: 0.10471  | Test Accuracy: 0.97\n",
      "Epoch 25 | Batch 468 | Train NLL: 0.15373 | Train Accuracy: 0.99  | Test NLL: 0.10301  | Test Accuracy: 0.97\n",
      "Epoch 26 | Batch 468 | Train NLL: 0.15266 | Train Accuracy: 0.99  | Test NLL: 0.10143  | Test Accuracy: 0.97\n",
      "Epoch 27 | Batch 468 | Train NLL: 0.15152 | Train Accuracy: 0.99  | Test NLL: 0.09995  | Test Accuracy: 0.97\n",
      "Epoch 28 | Batch 468 | Train NLL: 0.15052 | Train Accuracy: 0.99  | Test NLL: 0.09856  | Test Accuracy: 0.97\n",
      "Epoch 29 | Batch 468 | Train NLL: 0.14960 | Train Accuracy: 0.99  | Test NLL: 0.09722  | Test Accuracy: 0.97\n",
      "Epoch 30 | Batch 468 | Train NLL: 0.14846 | Train Accuracy: 0.99  | Test NLL: 0.09600  | Test Accuracy: 0.97\n",
      "Epoch 31 | Batch 468 | Train NLL: 0.14753 | Train Accuracy: 0.99  | Test NLL: 0.09485  | Test Accuracy: 0.97\n",
      "Epoch 32 | Batch 468 | Train NLL: 0.14666 | Train Accuracy: 0.99  | Test NLL: 0.09374  | Test Accuracy: 0.97\n",
      "Epoch 33 | Batch 468 | Train NLL: 0.14564 | Train Accuracy: 0.99  | Test NLL: 0.09271  | Test Accuracy: 0.97\n",
      "Epoch 34 | Batch 468 | Train NLL: 0.14467 | Train Accuracy: 0.99  | Test NLL: 0.09170  | Test Accuracy: 0.97\n",
      "Epoch 35 | Batch 468 | Train NLL: 0.14389 | Train Accuracy: 0.99  | Test NLL: 0.09076  | Test Accuracy: 0.97\n",
      "Epoch 36 | Batch 468 | Train NLL: 0.14301 | Train Accuracy: 0.99  | Test NLL: 0.08985  | Test Accuracy: 0.97\n",
      "Epoch 37 | Batch 468 | Train NLL: 0.14211 | Train Accuracy: 0.99  | Test NLL: 0.08903  | Test Accuracy: 0.97\n",
      "Epoch 38 | Batch 468 | Train NLL: 0.14120 | Train Accuracy: 0.99  | Test NLL: 0.08823  | Test Accuracy: 0.97\n",
      "Epoch 39 | Batch 468 | Train NLL: 0.14037 | Train Accuracy: 0.99  | Test NLL: 0.08747  | Test Accuracy: 0.97\n",
      "Epoch 40 | Batch 468 | Train NLL: 0.13964 | Train Accuracy: 0.99  | Test NLL: 0.08675  | Test Accuracy: 0.97\n",
      "Epoch 41 | Batch 468 | Train NLL: 0.13891 | Train Accuracy: 0.99  | Test NLL: 0.08606  | Test Accuracy: 0.97\n",
      "Epoch 42 | Batch 468 | Train NLL: 0.13813 | Train Accuracy: 0.99  | Test NLL: 0.08541  | Test Accuracy: 0.97\n",
      "Epoch 43 | Batch 468 | Train NLL: 0.13733 | Train Accuracy: 0.99  | Test NLL: 0.08476  | Test Accuracy: 0.97\n",
      "Epoch 44 | Batch 468 | Train NLL: 0.13670 | Train Accuracy: 0.99  | Test NLL: 0.08417  | Test Accuracy: 0.97\n",
      "Epoch 45 | Batch 468 | Train NLL: 0.13606 | Train Accuracy: 0.99  | Test NLL: 0.08357  | Test Accuracy: 0.98\n",
      "Epoch 46 | Batch 468 | Train NLL: 0.13538 | Train Accuracy: 0.99  | Test NLL: 0.08303  | Test Accuracy: 0.98\n",
      "Epoch 47 | Batch 468 | Train NLL: 0.13478 | Train Accuracy: 0.99  | Test NLL: 0.08252  | Test Accuracy: 0.98\n",
      "Epoch 48 | Batch 468 | Train NLL: 0.13406 | Train Accuracy: 0.99  | Test NLL: 0.08199  | Test Accuracy: 0.98\n",
      "Epoch 49 | Batch 468 | Train NLL: 0.13338 | Train Accuracy: 0.99  | Test NLL: 0.08150  | Test Accuracy: 0.98\n",
      "Epoch 50 | Batch 468 | Train NLL: 0.13269 | Train Accuracy: 0.99  | Test NLL: 0.08102  | Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "train_a_neural_net(optimize_args = {'mode': 'Nesterov', 'lr': .001, 'mu': .9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Batch 468 | Train NLL: 0.23781 | Train Accuracy: 0.98  | Test NLL: 0.11951  | Test Accuracy: 0.96\n",
      "Epoch 02 | Batch 468 | Train NLL: 0.21647 | Train Accuracy: 0.99  | Test NLL: 0.09735  | Test Accuracy: 0.97\n",
      "Epoch 03 | Batch 468 | Train NLL: 0.20542 | Train Accuracy: 0.99  | Test NLL: 0.09077  | Test Accuracy: 0.97\n",
      "Epoch 04 | Batch 468 | Train NLL: 0.17588 | Train Accuracy: 0.99  | Test NLL: 0.08892  | Test Accuracy: 0.97\n",
      "Epoch 05 | Batch 468 | Train NLL: 0.14915 | Train Accuracy: 0.99  | Test NLL: 0.08563  | Test Accuracy: 0.97\n",
      "Epoch 06 | Batch 468 | Train NLL: 0.13172 | Train Accuracy: 0.99  | Test NLL: 0.08606  | Test Accuracy: 0.97\n",
      "Epoch 07 | Batch 468 | Train NLL: 0.11468 | Train Accuracy: 0.99  | Test NLL: 0.08984  | Test Accuracy: 0.97\n",
      "Epoch 08 | Batch 468 | Train NLL: 0.08705 | Train Accuracy: 0.99  | Test NLL: 0.09253  | Test Accuracy: 0.97\n",
      "Epoch 09 | Batch 468 | Train NLL: 0.08447 | Train Accuracy: 0.99  | Test NLL: 0.08855  | Test Accuracy: 0.98\n",
      "Epoch 10 | Batch 468 | Train NLL: 0.04764 | Train Accuracy: 0.99  | Test NLL: 0.09470  | Test Accuracy: 0.98\n",
      "Epoch 11 | Batch 468 | Train NLL: 0.02366 | Train Accuracy: 0.99  | Test NLL: 0.09271  | Test Accuracy: 0.98\n",
      "Epoch 12 | Batch 468 | Train NLL: 0.00726 | Train Accuracy: 1.00  | Test NLL: 0.09252  | Test Accuracy: 0.98\n",
      "Epoch 13 | Batch 468 | Train NLL: 0.03741 | Train Accuracy: 0.99  | Test NLL: 0.11014  | Test Accuracy: 0.97\n",
      "Epoch 14 | Batch 468 | Train NLL: 0.02314 | Train Accuracy: 0.99  | Test NLL: 0.09993  | Test Accuracy: 0.98\n",
      "Epoch 15 | Batch 468 | Train NLL: 0.01358 | Train Accuracy: 0.99  | Test NLL: 0.09625  | Test Accuracy: 0.98\n",
      "Epoch 16 | Batch 468 | Train NLL: 0.00119 | Train Accuracy: 1.00  | Test NLL: 0.11919  | Test Accuracy: 0.97\n",
      "Epoch 17 | Batch 468 | Train NLL: 0.00337 | Train Accuracy: 1.00  | Test NLL: 0.11765  | Test Accuracy: 0.98\n",
      "Epoch 18 | Batch 468 | Train NLL: 0.00013 | Train Accuracy: 1.00  | Test NLL: 0.13304  | Test Accuracy: 0.97\n",
      "Epoch 19 | Batch 468 | Train NLL: 0.00478 | Train Accuracy: 1.00  | Test NLL: 0.11357  | Test Accuracy: 0.98\n",
      "Epoch 20 | Batch 468 | Train NLL: 0.00219 | Train Accuracy: 1.00  | Test NLL: 0.10635  | Test Accuracy: 0.98\n",
      "Epoch 21 | Batch 468 | Train NLL: 0.00066 | Train Accuracy: 1.00  | Test NLL: 0.13726  | Test Accuracy: 0.97\n",
      "Epoch 22 | Batch 468 | Train NLL: 0.01283 | Train Accuracy: 0.99  | Test NLL: 0.12072  | Test Accuracy: 0.98\n",
      "Epoch 23 | Batch 468 | Train NLL: 0.00239 | Train Accuracy: 1.00  | Test NLL: 0.11318  | Test Accuracy: 0.98\n",
      "Epoch 24 | Batch 468 | Train NLL: 0.00248 | Train Accuracy: 1.00  | Test NLL: 0.13440  | Test Accuracy: 0.97\n",
      "Epoch 25 | Batch 468 | Train NLL: 0.00157 | Train Accuracy: 1.00  | Test NLL: 0.11871  | Test Accuracy: 0.98\n",
      "Epoch 26 | Batch 468 | Train NLL: 0.00237 | Train Accuracy: 1.00  | Test NLL: 0.11892  | Test Accuracy: 0.98\n",
      "Epoch 27 | Batch 468 | Train NLL: 0.00083 | Train Accuracy: 1.00  | Test NLL: 0.13075  | Test Accuracy: 0.98\n",
      "Epoch 28 | Batch 468 | Train NLL: 0.00042 | Train Accuracy: 1.00  | Test NLL: 0.12820  | Test Accuracy: 0.98\n",
      "Epoch 29 | Batch 468 | Train NLL: 0.01178 | Train Accuracy: 0.99  | Test NLL: 0.13039  | Test Accuracy: 0.98\n",
      "Epoch 30 | Batch 468 | Train NLL: 0.02849 | Train Accuracy: 0.99  | Test NLL: 0.13127  | Test Accuracy: 0.98\n",
      "Epoch 31 | Batch 468 | Train NLL: 0.00006 | Train Accuracy: 1.00  | Test NLL: 0.11386  | Test Accuracy: 0.98\n",
      "Epoch 32 | Batch 468 | Train NLL: 0.01886 | Train Accuracy: 0.99  | Test NLL: 0.13196  | Test Accuracy: 0.98\n",
      "Epoch 33 | Batch 468 | Train NLL: 0.00049 | Train Accuracy: 1.00  | Test NLL: 0.13766  | Test Accuracy: 0.98\n",
      "Epoch 34 | Batch 468 | Train NLL: 0.00004 | Train Accuracy: 1.00  | Test NLL: 0.15580  | Test Accuracy: 0.98\n",
      "Epoch 35 | Batch 468 | Train NLL: 0.00000 | Train Accuracy: 1.00  | Test NLL: 0.14254  | Test Accuracy: 0.98\n",
      "Epoch 36 | Batch 468 | Train NLL: 0.00035 | Train Accuracy: 1.00  | Test NLL: 0.14592  | Test Accuracy: 0.98\n",
      "Epoch 37 | Batch 468 | Train NLL: 0.00005 | Train Accuracy: 1.00  | Test NLL: 0.14009  | Test Accuracy: 0.98\n",
      "Epoch 38 | Batch 468 | Train NLL: 0.00005 | Train Accuracy: 1.00  | Test NLL: 0.13078  | Test Accuracy: 0.98\n",
      "Epoch 39 | Batch 468 | Train NLL: 0.01748 | Train Accuracy: 0.99  | Test NLL: 0.14864  | Test Accuracy: 0.98\n",
      "Epoch 40 | Batch 468 | Train NLL: 0.00014 | Train Accuracy: 1.00  | Test NLL: 0.14195  | Test Accuracy: 0.98\n",
      "Epoch 41 | Batch 468 | Train NLL: 0.00134 | Train Accuracy: 1.00  | Test NLL: 0.15104  | Test Accuracy: 0.98\n",
      "Epoch 42 | Batch 468 | Train NLL: 0.00044 | Train Accuracy: 1.00  | Test NLL: 0.14911  | Test Accuracy: 0.98\n",
      "Epoch 43 | Batch 468 | Train NLL: 0.00027 | Train Accuracy: 1.00  | Test NLL: 0.13919  | Test Accuracy: 0.98\n",
      "Epoch 44 | Batch 468 | Train NLL: 0.06750 | Train Accuracy: 0.97  | Test NLL: 0.15785  | Test Accuracy: 0.98\n",
      "Epoch 45 | Batch 468 | Train NLL: 0.00006 | Train Accuracy: 1.00  | Test NLL: 0.15418  | Test Accuracy: 0.98\n",
      "Epoch 46 | Batch 468 | Train NLL: 0.00001 | Train Accuracy: 1.00  | Test NLL: 0.15148  | Test Accuracy: 0.98\n",
      "Epoch 47 | Batch 468 | Train NLL: 0.00006 | Train Accuracy: 1.00  | Test NLL: 0.14525  | Test Accuracy: 0.98\n",
      "Epoch 48 | Batch 468 | Train NLL: 0.00001 | Train Accuracy: 1.00  | Test NLL: 0.14452  | Test Accuracy: 0.98\n",
      "Epoch 49 | Batch 468 | Train NLL: 0.00010 | Train Accuracy: 1.00  | Test NLL: 0.15225  | Test Accuracy: 0.98\n",
      "Epoch 50 | Batch 468 | Train NLL: 0.00003 | Train Accuracy: 1.00  | Test NLL: 0.16044  | Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "train_a_neural_net(optimize_args = {'mode': 'Adam', 'lr': .001, 'beta1': .9, 'beta2': .999})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Feed-forward nets for image classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
