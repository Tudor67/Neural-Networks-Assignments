{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN-TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "tf.__version__: 1.11.0\n",
      "CUDA Version 9.0.176\r\n"
     ]
    }
   ],
   "source": [
    "print(f'sys.version: {sys.version}')\n",
    "print(f'tf.__version__: {tf.__version__}')\n",
    "!cat /usr/local/cuda/version.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "`./data/input.txt` contains 3 Harry Potter books (1st, 3rd, 4th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 2162018 characters\n"
     ]
    }
   ],
   "source": [
    "fi = open('./data/input.txt')\n",
    "text = fi.read()\n",
    "fi.close()\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Look at the first 1000 characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The unique characters in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'\"'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "'('    --->    6\n",
      "')'    --->    7\n",
      "'*'    --->    8\n",
      "','    --->    9\n",
      "'-'    --->   10\n",
      "'.'    --->   11\n",
      "'/'    --->   12\n",
      "'0'    --->   13\n",
      "'1'    --->   14\n",
      "'2'    --->   15\n",
      "'3'    --->   16\n",
      "'4'    --->   17\n",
      "'5'    --->   18\n",
      "'6'    --->   19\n",
      "'7'    --->   20\n",
      "'8'    --->   21\n",
      "'9'    --->   22\n",
      "':'    --->   23\n",
      "';'    --->   24\n",
      "'?'    --->   25\n",
      "'A'    --->   26\n",
      "'B'    --->   27\n",
      "'C'    --->   28\n",
      "'D'    --->   29\n"
     ]
    }
   ],
   "source": [
    "# Integer representation for each character\n",
    "for char,_ in zip(char2idx, range(30)):\n",
    "    print(f'{repr(char):6s} ---> {char2idx[char]:4d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and ---- characters mapped to int ---- > [33 56 73 73 80  1 41 70 75 75 60 73  1 56 69 59]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 16 characters from the text are mapped to integers\n",
    "print(f'{text[:16]} ---- characters mapped to int ---- > {text_as_int[:16]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating training examples and targets\n",
    "Each training example will contain `seq_length` characters from the text. The corresponding targets contain the same  length of text, except shifted one character to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of nu\"\n",
      "'mber four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They'\n",
      "\" were the last people you'd expect to be involved in anything strange or mysterious, because they jus\"\n",
      "\"t didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which ma\"\n",
      "'de drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. '\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples / targets\n",
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in chunks.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input and target texts from this chunk\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of n\"\n",
      "Target data: \"arry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of nu\"\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first 10 values of the first example\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating batches and shuffling them using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if tf.test.is_gpu_available():\n",
    "            self.lstm = tf.keras.layers.CuDNNLSTM(self.units,\n",
    "                                                  return_sequences=True, \n",
    "                                                  recurrent_initializer='glorot_uniform',\n",
    "                                                  stateful=True)\n",
    "        else:\n",
    "            self.lstm = tf.keras.layers.LSTM(self.units, \n",
    "                                             return_sequences=True,\n",
    "                                             recurrent_activation='sigmoid', \n",
    "                                             recurrent_initializer='glorot_uniform', \n",
    "                                             stateful=True)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "\n",
    "        # output at every time step\n",
    "        # output shape == (batch_size, seq_length, hidden_size) \n",
    "        output = self.lstm(embedding)\n",
    "\n",
    "        # The dense layer will output predictions for every time_steps(seq_length)\n",
    "        # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n",
    "        prediction = self.fc(output)\n",
    "\n",
    "        # states will be used to pass at every step to the model while training\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "units = 512\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Instantiate optimizer  and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using adam optimizer with default arguments\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# Checkpoint instance\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  22784     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       multiple                  1576960   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  45657     \n",
      "=================================================================\n",
      "Total params: 1,645,401\n",
      "Trainable params: 1,645,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(EPOCHS=10):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        # initializing the hidden state at the start of every epoch\n",
    "        # initially hidden is None\n",
    "        hidden = model.reset_states()\n",
    "\n",
    "        for (batch, (inp, target)) in enumerate(dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # feeding the hidden state back into the model\n",
    "                # This is the interesting step\n",
    "                predictions = model(inp)\n",
    "                loss = loss_function(target, predictions)\n",
    "\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "        # saving (checkpoint) the model every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4889\n",
      "Epoch 1 Batch 100 Loss 2.4287\n",
      "Epoch 1 Batch 200 Loss 2.0609\n",
      "Epoch 1 Batch 300 Loss 1.8467\n",
      "Epoch 1 Loss 1.8402\n",
      "Time taken for 1 epoch 25.274771451950073 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8853\n",
      "Epoch 2 Batch 100 Loss 1.7140\n",
      "Epoch 2 Batch 200 Loss 1.6073\n",
      "Epoch 2 Batch 300 Loss 1.5782\n",
      "Epoch 2 Loss 1.6051\n",
      "Time taken for 1 epoch 24.304714918136597 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5724\n",
      "Epoch 3 Batch 100 Loss 1.5270\n",
      "Epoch 3 Batch 200 Loss 1.5086\n",
      "Epoch 3 Batch 300 Loss 1.4021\n",
      "Epoch 3 Loss 1.3950\n",
      "Time taken for 1 epoch 24.57616400718689 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.4677\n",
      "Epoch 4 Batch 100 Loss 1.4006\n",
      "Epoch 4 Batch 200 Loss 1.3581\n",
      "Epoch 4 Batch 300 Loss 1.3453\n",
      "Epoch 4 Loss 1.3637\n",
      "Time taken for 1 epoch 24.421303272247314 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4008\n",
      "Epoch 5 Batch 100 Loss 1.3350\n",
      "Epoch 5 Batch 200 Loss 1.3138\n",
      "Epoch 5 Batch 300 Loss 1.3027\n",
      "Epoch 5 Loss 1.3430\n",
      "Time taken for 1 epoch 24.512234449386597 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2917\n",
      "Epoch 6 Batch 100 Loss 1.3202\n",
      "Epoch 6 Batch 200 Loss 1.2901\n",
      "Epoch 6 Batch 300 Loss 1.2921\n",
      "Epoch 6 Loss 1.2651\n",
      "Time taken for 1 epoch 23.95991802215576 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2702\n",
      "Epoch 7 Batch 100 Loss 1.2228\n",
      "Epoch 7 Batch 200 Loss 1.2787\n",
      "Epoch 7 Batch 300 Loss 1.2419\n",
      "Epoch 7 Loss 1.2799\n",
      "Time taken for 1 epoch 24.31678080558777 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2326\n",
      "Epoch 8 Batch 100 Loss 1.2400\n",
      "Epoch 8 Batch 200 Loss 1.2253\n",
      "Epoch 8 Batch 300 Loss 1.1942\n",
      "Epoch 8 Loss 1.2247\n",
      "Time taken for 1 epoch 24.18035054206848 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.2394\n",
      "Epoch 9 Batch 100 Loss 1.2855\n",
      "Epoch 9 Batch 200 Loss 1.2206\n",
      "Epoch 9 Batch 300 Loss 1.1947\n",
      "Epoch 9 Loss 1.1824\n",
      "Time taken for 1 epoch 24.00411343574524 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.2118\n",
      "Epoch 10 Batch 100 Loss 1.1766\n",
      "Epoch 10 Batch 200 Loss 1.1598\n",
      "Epoch 10 Batch 300 Loss 1.1769\n",
      "Epoch 10 Loss 1.1453\n",
      "Time taken for 1 epoch 24.316882610321045 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.1859\n",
      "Epoch 11 Batch 100 Loss 1.1226\n",
      "Epoch 11 Batch 200 Loss 1.1721\n",
      "Epoch 11 Batch 300 Loss 1.1777\n",
      "Epoch 11 Loss 1.1773\n",
      "Time taken for 1 epoch 23.929895401000977 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.1621\n",
      "Epoch 12 Batch 100 Loss 1.1929\n",
      "Epoch 12 Batch 200 Loss 1.1415\n",
      "Epoch 12 Batch 300 Loss 1.1238\n",
      "Epoch 12 Loss 1.1720\n",
      "Time taken for 1 epoch 24.24251437187195 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.1454\n",
      "Epoch 13 Batch 100 Loss 1.1390\n",
      "Epoch 13 Batch 200 Loss 1.1768\n",
      "Epoch 13 Batch 300 Loss 1.1632\n",
      "Epoch 13 Loss 1.1401\n",
      "Time taken for 1 epoch 24.294461488723755 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.1460\n",
      "Epoch 14 Batch 100 Loss 1.1018\n",
      "Epoch 14 Batch 200 Loss 1.0869\n",
      "Epoch 14 Batch 300 Loss 1.1325\n",
      "Epoch 14 Loss 1.1222\n",
      "Time taken for 1 epoch 23.933595657348633 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.1454\n",
      "Epoch 15 Batch 100 Loss 1.1447\n",
      "Epoch 15 Batch 200 Loss 1.0843\n",
      "Epoch 15 Batch 300 Loss 1.1154\n",
      "Epoch 15 Loss 1.1267\n",
      "Time taken for 1 epoch 24.019805192947388 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.1394\n",
      "Epoch 16 Batch 100 Loss 1.1798\n",
      "Epoch 16 Batch 200 Loss 1.1212\n",
      "Epoch 16 Batch 300 Loss 1.0839\n",
      "Epoch 16 Loss 1.1164\n",
      "Time taken for 1 epoch 23.97158646583557 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.1016\n",
      "Epoch 17 Batch 100 Loss 1.0839\n",
      "Epoch 17 Batch 200 Loss 1.0954\n",
      "Epoch 17 Batch 300 Loss 1.1141\n",
      "Epoch 17 Loss 1.1019\n",
      "Time taken for 1 epoch 23.947675704956055 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.0999\n",
      "Epoch 18 Batch 100 Loss 1.0942\n",
      "Epoch 18 Batch 200 Loss 1.0976\n",
      "Epoch 18 Batch 300 Loss 1.0862\n",
      "Epoch 18 Loss 1.0522\n",
      "Time taken for 1 epoch 23.9636709690094 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.0826\n",
      "Epoch 19 Batch 100 Loss 1.1214\n",
      "Epoch 19 Batch 200 Loss 1.0916\n",
      "Epoch 19 Batch 300 Loss 1.1174\n",
      "Epoch 19 Loss 1.0784\n",
      "Time taken for 1 epoch 24.035525798797607 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0866\n",
      "Epoch 20 Batch 100 Loss 1.0716\n",
      "Epoch 20 Batch 200 Loss 1.0928\n",
      "Epoch 20 Batch 300 Loss 1.0654\n",
      "Epoch 20 Loss 1.0969\n",
      "Time taken for 1 epoch 24.130158185958862 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.1139\n",
      "Epoch 21 Batch 100 Loss 1.0487\n",
      "Epoch 21 Batch 200 Loss 1.0608\n",
      "Epoch 21 Batch 300 Loss 1.0478\n",
      "Epoch 21 Loss 1.0749\n",
      "Time taken for 1 epoch 24.023072242736816 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.0439\n",
      "Epoch 22 Batch 100 Loss 1.0277\n",
      "Epoch 22 Batch 200 Loss 1.0659\n",
      "Epoch 22 Batch 300 Loss 1.0699\n",
      "Epoch 22 Loss 1.0881\n",
      "Time taken for 1 epoch 23.97456121444702 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.0523\n",
      "Epoch 23 Batch 100 Loss 1.0131\n",
      "Epoch 23 Batch 200 Loss 1.0519\n",
      "Epoch 23 Batch 300 Loss 1.0598\n",
      "Epoch 23 Loss 1.0179\n",
      "Time taken for 1 epoch 24.09767484664917 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.0240\n",
      "Epoch 24 Batch 100 Loss 1.0446\n",
      "Epoch 24 Batch 200 Loss 1.0934\n",
      "Epoch 24 Batch 300 Loss 1.0727\n",
      "Epoch 24 Loss 1.0557\n",
      "Time taken for 1 epoch 24.01062822341919 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.0158\n",
      "Epoch 25 Batch 100 Loss 1.0273\n",
      "Epoch 25 Batch 200 Loss 1.0316\n",
      "Epoch 25 Batch 300 Loss 1.0307\n",
      "Epoch 25 Loss 1.0432\n",
      "Time taken for 1 epoch 23.985883474349976 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.0338\n",
      "Epoch 26 Batch 100 Loss 1.0829\n",
      "Epoch 26 Batch 200 Loss 1.0566\n",
      "Epoch 26 Batch 300 Loss 1.0128\n",
      "Epoch 26 Loss 1.0278\n",
      "Time taken for 1 epoch 23.940481185913086 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.0147\n",
      "Epoch 27 Batch 100 Loss 0.9987\n",
      "Epoch 27 Batch 200 Loss 1.0272\n",
      "Epoch 27 Batch 300 Loss 1.0303\n",
      "Epoch 27 Loss 1.0252\n",
      "Time taken for 1 epoch 24.04940962791443 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.0078\n",
      "Epoch 28 Batch 100 Loss 1.0912\n",
      "Epoch 28 Batch 200 Loss 1.0164\n",
      "Epoch 28 Batch 300 Loss 1.0347\n",
      "Epoch 28 Loss 1.0228\n",
      "Time taken for 1 epoch 23.939640760421753 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.0198\n",
      "Epoch 29 Batch 100 Loss 1.0255\n",
      "Epoch 29 Batch 200 Loss 1.0351\n",
      "Epoch 29 Batch 300 Loss 1.0064\n",
      "Epoch 29 Loss 1.0307\n",
      "Time taken for 1 epoch 24.017730236053467 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.0187\n",
      "Epoch 30 Batch 100 Loss 1.0155\n",
      "Epoch 30 Batch 200 Loss 1.0480\n",
      "Epoch 30 Batch 300 Loss 1.0096\n",
      "Epoch 30 Loss 1.0271\n",
      "Time taken for 1 epoch 24.13757348060608 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.9876\n",
      "Epoch 31 Batch 100 Loss 1.0122\n",
      "Epoch 31 Batch 200 Loss 1.0105\n",
      "Epoch 31 Batch 300 Loss 1.0333\n",
      "Epoch 31 Loss 0.9940\n",
      "Time taken for 1 epoch 23.91358995437622 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.9957\n",
      "Epoch 32 Batch 100 Loss 0.9815\n",
      "Epoch 32 Batch 200 Loss 0.9703\n",
      "Epoch 32 Batch 300 Loss 0.9900\n",
      "Epoch 32 Loss 1.0026\n",
      "Time taken for 1 epoch 23.910906076431274 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.9874\n",
      "Epoch 33 Batch 100 Loss 0.9840\n",
      "Epoch 33 Batch 200 Loss 0.9855\n",
      "Epoch 33 Batch 300 Loss 0.9750\n",
      "Epoch 33 Loss 0.9928\n",
      "Time taken for 1 epoch 23.99430012702942 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.9716\n",
      "Epoch 34 Batch 100 Loss 1.0002\n",
      "Epoch 34 Batch 200 Loss 0.9693\n",
      "Epoch 34 Batch 300 Loss 0.9891\n",
      "Epoch 34 Loss 0.9917\n",
      "Time taken for 1 epoch 23.925962209701538 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.9305\n",
      "Epoch 35 Batch 100 Loss 0.9439\n",
      "Epoch 35 Batch 200 Loss 0.9948\n",
      "Epoch 35 Batch 300 Loss 0.9880\n",
      "Epoch 35 Loss 1.0037\n",
      "Time taken for 1 epoch 24.155985593795776 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.9359\n",
      "Epoch 36 Batch 100 Loss 0.9650\n",
      "Epoch 36 Batch 200 Loss 0.9868\n",
      "Epoch 36 Batch 300 Loss 1.0059\n",
      "Epoch 36 Loss 1.0095\n",
      "Time taken for 1 epoch 23.907240390777588 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.9867\n",
      "Epoch 37 Batch 100 Loss 0.9833\n",
      "Epoch 37 Batch 200 Loss 0.9508\n",
      "Epoch 37 Batch 300 Loss 0.9957\n",
      "Epoch 37 Loss 0.9818\n",
      "Time taken for 1 epoch 24.01644539833069 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.9674\n",
      "Epoch 38 Batch 100 Loss 0.9743\n",
      "Epoch 38 Batch 200 Loss 0.9937\n",
      "Epoch 38 Batch 300 Loss 0.9620\n",
      "Epoch 38 Loss 0.9520\n",
      "Time taken for 1 epoch 23.89143419265747 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.9516\n",
      "Epoch 39 Batch 100 Loss 0.9647\n",
      "Epoch 39 Batch 200 Loss 0.9675\n",
      "Epoch 39 Batch 300 Loss 1.0017\n",
      "Epoch 39 Loss 0.9591\n",
      "Time taken for 1 epoch 23.88122057914734 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.9765\n",
      "Epoch 40 Batch 100 Loss 0.9520\n",
      "Epoch 40 Batch 200 Loss 0.9711\n",
      "Epoch 40 Batch 300 Loss 0.9670\n",
      "Epoch 40 Loss 0.9701\n",
      "Time taken for 1 epoch 23.99869990348816 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.9404\n",
      "Epoch 41 Batch 100 Loss 0.9329\n",
      "Epoch 41 Batch 200 Loss 0.9571\n",
      "Epoch 41 Batch 300 Loss 0.9471\n",
      "Epoch 41 Loss 0.9513\n",
      "Time taken for 1 epoch 23.976662158966064 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.9164\n",
      "Epoch 42 Batch 100 Loss 0.9320\n",
      "Epoch 42 Batch 200 Loss 0.9733\n",
      "Epoch 42 Batch 300 Loss 0.9533\n",
      "Epoch 42 Loss 0.9507\n",
      "Time taken for 1 epoch 23.939031839370728 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.9419\n",
      "Epoch 43 Batch 100 Loss 0.9453\n",
      "Epoch 43 Batch 200 Loss 0.9587\n",
      "Epoch 43 Batch 300 Loss 0.9618\n",
      "Epoch 43 Loss 0.9707\n",
      "Time taken for 1 epoch 24.02593970298767 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.9318\n",
      "Epoch 44 Batch 100 Loss 0.9363\n",
      "Epoch 44 Batch 200 Loss 0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 300 Loss 0.9432\n",
      "Epoch 44 Loss 0.9186\n",
      "Time taken for 1 epoch 23.932286024093628 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.9418\n",
      "Epoch 45 Batch 100 Loss 0.9506\n",
      "Epoch 45 Batch 200 Loss 0.9101\n",
      "Epoch 45 Batch 300 Loss 0.9506\n",
      "Epoch 45 Loss 0.9441\n",
      "Time taken for 1 epoch 24.099271059036255 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.9141\n",
      "Epoch 46 Batch 100 Loss 0.9378\n",
      "Epoch 46 Batch 200 Loss 0.9628\n",
      "Epoch 46 Batch 300 Loss 0.9403\n",
      "Epoch 46 Loss 0.9463\n",
      "Time taken for 1 epoch 23.94114089012146 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.9334\n",
      "Epoch 47 Batch 100 Loss 0.9311\n",
      "Epoch 47 Batch 200 Loss 0.9363\n",
      "Epoch 47 Batch 300 Loss 0.8969\n",
      "Epoch 47 Loss 0.9173\n",
      "Time taken for 1 epoch 23.98555874824524 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.9099\n",
      "Epoch 48 Batch 100 Loss 0.9189\n",
      "Epoch 48 Batch 200 Loss 0.9389\n",
      "Epoch 48 Batch 300 Loss 0.9336\n",
      "Epoch 48 Loss 0.9430\n",
      "Time taken for 1 epoch 23.911340713500977 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.9198\n",
      "Epoch 49 Batch 100 Loss 0.8979\n",
      "Epoch 49 Batch 200 Loss 0.9213\n",
      "Epoch 49 Batch 300 Loss 0.8921\n",
      "Epoch 49 Loss 0.9336\n",
      "Time taken for 1 epoch 24.080400466918945 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.9067\n",
      "Epoch 50 Batch 100 Loss 0.9356\n",
      "Epoch 50 Batch 200 Loss 0.9211\n",
      "Epoch 50 Batch 300 Loss 0.9371\n",
      "Epoch 50 Loss 0.9517\n",
      "Time taken for 1 epoch 24.036492109298706 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.8826\n",
      "Epoch 51 Batch 100 Loss 0.9085\n",
      "Epoch 51 Batch 200 Loss 0.9211\n",
      "Epoch 51 Batch 300 Loss 0.9684\n",
      "Epoch 51 Loss 0.9128\n",
      "Time taken for 1 epoch 24.088106870651245 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.8928\n",
      "Epoch 52 Batch 100 Loss 0.9113\n",
      "Epoch 52 Batch 200 Loss 0.9182\n",
      "Epoch 52 Batch 300 Loss 0.9194\n",
      "Epoch 52 Loss 0.9048\n",
      "Time taken for 1 epoch 24.093690395355225 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.8691\n",
      "Epoch 53 Batch 100 Loss 0.9314\n",
      "Epoch 53 Batch 200 Loss 0.8959\n",
      "Epoch 53 Batch 300 Loss 0.8977\n",
      "Epoch 53 Loss 0.9225\n",
      "Time taken for 1 epoch 23.948691368103027 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.8924\n",
      "Epoch 54 Batch 100 Loss 0.9308\n",
      "Epoch 54 Batch 200 Loss 0.9164\n",
      "Epoch 54 Batch 300 Loss 0.9179\n",
      "Epoch 54 Loss 0.9072\n",
      "Time taken for 1 epoch 24.065277099609375 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.9165\n",
      "Epoch 55 Batch 100 Loss 0.9032\n",
      "Epoch 55 Batch 200 Loss 0.9139\n",
      "Epoch 55 Batch 300 Loss 0.9218\n",
      "Epoch 55 Loss 0.9138\n",
      "Time taken for 1 epoch 24.197408437728882 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.8847\n",
      "Epoch 56 Batch 100 Loss 0.9107\n",
      "Epoch 56 Batch 200 Loss 0.9013\n",
      "Epoch 56 Batch 300 Loss 0.9209\n",
      "Epoch 56 Loss 0.9262\n",
      "Time taken for 1 epoch 24.038220643997192 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.9025\n",
      "Epoch 57 Batch 100 Loss 0.9205\n",
      "Epoch 57 Batch 200 Loss 0.9015\n",
      "Epoch 57 Batch 300 Loss 0.9076\n",
      "Epoch 57 Loss 0.8996\n",
      "Time taken for 1 epoch 23.848658084869385 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.8901\n",
      "Epoch 58 Batch 100 Loss 0.8871\n",
      "Epoch 58 Batch 200 Loss 0.9233\n",
      "Epoch 58 Batch 300 Loss 0.9131\n",
      "Epoch 58 Loss 0.9400\n",
      "Time taken for 1 epoch 23.940019130706787 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.8805\n",
      "Epoch 59 Batch 100 Loss 0.8877\n",
      "Epoch 59 Batch 200 Loss 0.8930\n",
      "Epoch 59 Batch 300 Loss 0.9203\n",
      "Epoch 59 Loss 0.9356\n",
      "Time taken for 1 epoch 24.022106409072876 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.8742\n",
      "Epoch 60 Batch 100 Loss 0.8861\n",
      "Epoch 60 Batch 200 Loss 0.9188\n",
      "Epoch 60 Batch 300 Loss 0.9330\n",
      "Epoch 60 Loss 0.9210\n",
      "Time taken for 1 epoch 24.124565601348877 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.8580\n",
      "Epoch 61 Batch 100 Loss 0.9239\n",
      "Epoch 61 Batch 200 Loss 0.9011\n",
      "Epoch 61 Batch 300 Loss 0.8946\n",
      "Epoch 61 Loss 0.9219\n",
      "Time taken for 1 epoch 23.96848154067993 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.8950\n",
      "Epoch 62 Batch 100 Loss 0.8878\n",
      "Epoch 62 Batch 200 Loss 0.8782\n",
      "Epoch 62 Batch 300 Loss 0.9157\n",
      "Epoch 62 Loss 0.8968\n",
      "Time taken for 1 epoch 23.951735496520996 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.8914\n",
      "Epoch 63 Batch 100 Loss 0.8806\n",
      "Epoch 63 Batch 200 Loss 0.9085\n",
      "Epoch 63 Batch 300 Loss 0.8823\n",
      "Epoch 63 Loss 0.9028\n",
      "Time taken for 1 epoch 24.00161838531494 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.8409\n",
      "Epoch 64 Batch 100 Loss 0.9010\n",
      "Epoch 64 Batch 200 Loss 0.9035\n",
      "Epoch 64 Batch 300 Loss 0.9062\n",
      "Epoch 64 Loss 0.8620\n",
      "Time taken for 1 epoch 24.156307458877563 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.8471\n",
      "Epoch 65 Batch 100 Loss 0.8835\n",
      "Epoch 65 Batch 200 Loss 0.8930\n",
      "Epoch 65 Batch 300 Loss 0.9202\n",
      "Epoch 65 Loss 0.8828\n",
      "Time taken for 1 epoch 24.134169101715088 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.8611\n",
      "Epoch 66 Batch 100 Loss 0.8931\n",
      "Epoch 66 Batch 200 Loss 0.9089\n",
      "Epoch 66 Batch 300 Loss 0.9396\n",
      "Epoch 66 Loss 0.9113\n",
      "Time taken for 1 epoch 23.88421607017517 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.8472\n",
      "Epoch 67 Batch 100 Loss 0.8991\n",
      "Epoch 67 Batch 200 Loss 0.8704\n",
      "Epoch 67 Batch 300 Loss 0.8837\n",
      "Epoch 67 Loss 0.9089\n",
      "Time taken for 1 epoch 23.967795610427856 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.8628\n",
      "Epoch 68 Batch 100 Loss 0.8777\n",
      "Epoch 68 Batch 200 Loss 0.8819\n",
      "Epoch 68 Batch 300 Loss 0.8973\n",
      "Epoch 68 Loss 0.8922\n",
      "Time taken for 1 epoch 23.857895612716675 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.8770\n",
      "Epoch 69 Batch 100 Loss 0.8575\n",
      "Epoch 69 Batch 200 Loss 0.8848\n",
      "Epoch 69 Batch 300 Loss 0.8970\n",
      "Epoch 69 Loss 0.8698\n",
      "Time taken for 1 epoch 24.09128499031067 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.8470\n",
      "Epoch 70 Batch 100 Loss 0.9080\n",
      "Epoch 70 Batch 200 Loss 0.8783\n",
      "Epoch 70 Batch 300 Loss 0.8640\n",
      "Epoch 70 Loss 0.8915\n",
      "Time taken for 1 epoch 24.270922899246216 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.8449\n",
      "Epoch 71 Batch 100 Loss 0.8764\n",
      "Epoch 71 Batch 200 Loss 0.9101\n",
      "Epoch 71 Batch 300 Loss 0.8718\n",
      "Epoch 71 Loss 0.9091\n",
      "Time taken for 1 epoch 23.917807817459106 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.8472\n",
      "Epoch 72 Batch 100 Loss 0.8440\n",
      "Epoch 72 Batch 200 Loss 0.9024\n",
      "Epoch 72 Batch 300 Loss 0.8912\n",
      "Epoch 72 Loss 0.8957\n",
      "Time taken for 1 epoch 23.98077392578125 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.8369\n",
      "Epoch 73 Batch 100 Loss 0.8967\n",
      "Epoch 73 Batch 200 Loss 0.8728\n",
      "Epoch 73 Batch 300 Loss 0.9019\n",
      "Epoch 73 Loss 0.9052\n",
      "Time taken for 1 epoch 23.940765380859375 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.8580\n",
      "Epoch 74 Batch 100 Loss 0.8499\n",
      "Epoch 74 Batch 200 Loss 0.8791\n",
      "Epoch 74 Batch 300 Loss 0.9013\n",
      "Epoch 74 Loss 0.9127\n",
      "Time taken for 1 epoch 23.925673723220825 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.8026\n",
      "Epoch 75 Batch 100 Loss 0.8749\n",
      "Epoch 75 Batch 200 Loss 0.8665\n",
      "Epoch 75 Batch 300 Loss 0.8806\n",
      "Epoch 75 Loss 0.8923\n",
      "Time taken for 1 epoch 24.249324321746826 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.8479\n",
      "Epoch 76 Batch 100 Loss 0.8944\n",
      "Epoch 76 Batch 200 Loss 0.8676\n",
      "Epoch 76 Batch 300 Loss 0.8678\n",
      "Epoch 76 Loss 0.8563\n",
      "Time taken for 1 epoch 23.962750673294067 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.8596\n",
      "Epoch 77 Batch 100 Loss 0.9027\n",
      "Epoch 77 Batch 200 Loss 0.8727\n",
      "Epoch 77 Batch 300 Loss 0.8984\n",
      "Epoch 77 Loss 0.8865\n",
      "Time taken for 1 epoch 23.99431824684143 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.8484\n",
      "Epoch 78 Batch 100 Loss 0.8665\n",
      "Epoch 78 Batch 200 Loss 0.8810\n",
      "Epoch 78 Batch 300 Loss 0.8912\n",
      "Epoch 78 Loss 0.8872\n",
      "Time taken for 1 epoch 23.845755100250244 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.8254\n",
      "Epoch 79 Batch 100 Loss 0.8544\n",
      "Epoch 79 Batch 200 Loss 0.8661\n",
      "Epoch 79 Batch 300 Loss 0.9017\n",
      "Epoch 79 Loss 0.8980\n",
      "Time taken for 1 epoch 23.794066667556763 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.8542\n",
      "Epoch 80 Batch 100 Loss 0.8594\n",
      "Epoch 80 Batch 200 Loss 0.8627\n",
      "Epoch 80 Batch 300 Loss 0.8818\n",
      "Epoch 80 Loss 0.8736\n",
      "Time taken for 1 epoch 23.79295825958252 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.8602\n",
      "Epoch 81 Batch 100 Loss 0.8576\n",
      "Epoch 81 Batch 200 Loss 0.8415\n",
      "Epoch 81 Batch 300 Loss 0.8708\n",
      "Epoch 81 Loss 0.8514\n",
      "Time taken for 1 epoch 23.80807328224182 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.8647\n",
      "Epoch 82 Batch 100 Loss 0.8695\n",
      "Epoch 82 Batch 200 Loss 0.8605\n",
      "Epoch 82 Batch 300 Loss 0.8682\n",
      "Epoch 82 Loss 0.8733\n",
      "Time taken for 1 epoch 23.734383583068848 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.8385\n",
      "Epoch 83 Batch 100 Loss 0.8803\n",
      "Epoch 83 Batch 200 Loss 0.8669\n",
      "Epoch 83 Batch 300 Loss 0.8578\n",
      "Epoch 83 Loss 0.8798\n",
      "Time taken for 1 epoch 23.761531829833984 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.8790\n",
      "Epoch 84 Batch 100 Loss 0.8306\n",
      "Epoch 84 Batch 200 Loss 0.8841\n",
      "Epoch 84 Batch 300 Loss 0.8752\n",
      "Epoch 84 Loss 0.8295\n",
      "Time taken for 1 epoch 23.836763620376587 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.8310\n",
      "Epoch 85 Batch 100 Loss 0.8514\n",
      "Epoch 85 Batch 200 Loss 0.8292\n",
      "Epoch 85 Batch 300 Loss 0.8628\n",
      "Epoch 85 Loss 0.8698\n",
      "Time taken for 1 epoch 23.81281614303589 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.8220\n",
      "Epoch 86 Batch 100 Loss 0.8713\n",
      "Epoch 86 Batch 200 Loss 0.8728\n",
      "Epoch 86 Batch 300 Loss 0.8957\n",
      "Epoch 86 Loss 0.9013\n",
      "Time taken for 1 epoch 23.74521017074585 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.8562\n",
      "Epoch 87 Batch 100 Loss 0.8391\n",
      "Epoch 87 Batch 200 Loss 0.8318\n",
      "Epoch 87 Batch 300 Loss 0.8756\n",
      "Epoch 87 Loss 0.8817\n",
      "Time taken for 1 epoch 23.91914987564087 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 0 Loss 0.8350\n",
      "Epoch 88 Batch 100 Loss 0.8533\n",
      "Epoch 88 Batch 200 Loss 0.8927\n",
      "Epoch 88 Batch 300 Loss 0.8705\n",
      "Epoch 88 Loss 0.8552\n",
      "Time taken for 1 epoch 23.76588273048401 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.8508\n",
      "Epoch 89 Batch 100 Loss 0.8815\n",
      "Epoch 89 Batch 200 Loss 0.8684\n",
      "Epoch 89 Batch 300 Loss 0.8502\n",
      "Epoch 89 Loss 0.8822\n",
      "Time taken for 1 epoch 23.278547286987305 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.8397\n",
      "Epoch 90 Batch 100 Loss 0.8327\n",
      "Epoch 90 Batch 200 Loss 0.8705\n",
      "Epoch 90 Batch 300 Loss 0.8466\n",
      "Epoch 90 Loss 0.8670\n",
      "Time taken for 1 epoch 24.09212040901184 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.8588\n",
      "Epoch 91 Batch 100 Loss 0.8663\n",
      "Epoch 91 Batch 200 Loss 0.8715\n",
      "Epoch 91 Batch 300 Loss 0.8917\n",
      "Epoch 91 Loss 0.8516\n",
      "Time taken for 1 epoch 24.06751799583435 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.8573\n",
      "Epoch 92 Batch 100 Loss 0.8645\n",
      "Epoch 92 Batch 200 Loss 0.9080\n",
      "Epoch 92 Batch 300 Loss 0.8475\n",
      "Epoch 92 Loss 0.8626\n",
      "Time taken for 1 epoch 24.31036376953125 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.8518\n",
      "Epoch 93 Batch 100 Loss 0.8468\n",
      "Epoch 93 Batch 200 Loss 0.8560\n",
      "Epoch 93 Batch 300 Loss 0.8825\n",
      "Epoch 93 Loss 0.8802\n",
      "Time taken for 1 epoch 24.317376136779785 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.8102\n",
      "Epoch 94 Batch 100 Loss 0.8654\n",
      "Epoch 94 Batch 200 Loss 0.8415\n",
      "Epoch 94 Batch 300 Loss 0.8998\n",
      "Epoch 94 Loss 0.8417\n",
      "Time taken for 1 epoch 24.283776998519897 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.8327\n",
      "Epoch 95 Batch 100 Loss 0.8228\n",
      "Epoch 95 Batch 200 Loss 0.8342\n",
      "Epoch 95 Batch 300 Loss 0.8602\n",
      "Epoch 95 Loss 0.8914\n",
      "Time taken for 1 epoch 24.558687448501587 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.8330\n",
      "Epoch 96 Batch 100 Loss 0.8461\n",
      "Epoch 96 Batch 200 Loss 0.8514\n",
      "Epoch 96 Batch 300 Loss 0.8587\n",
      "Epoch 96 Loss 0.8334\n",
      "Time taken for 1 epoch 24.133058547973633 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.8556\n",
      "Epoch 97 Batch 100 Loss 0.8599\n",
      "Epoch 97 Batch 200 Loss 0.8827\n",
      "Epoch 97 Batch 300 Loss 0.8764\n",
      "Epoch 97 Loss 0.8445\n",
      "Time taken for 1 epoch 24.009847164154053 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.8168\n",
      "Epoch 98 Batch 100 Loss 0.8532\n",
      "Epoch 98 Batch 200 Loss 0.8475\n",
      "Epoch 98 Batch 300 Loss 0.8736\n",
      "Epoch 98 Loss 0.8931\n",
      "Time taken for 1 epoch 24.92194628715515 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.8486\n",
      "Epoch 99 Batch 100 Loss 0.8560\n",
      "Epoch 99 Batch 200 Loss 0.8539\n",
      "Epoch 99 Batch 300 Loss 0.8753\n",
      "Epoch 99 Loss 0.8888\n",
      "Time taken for 1 epoch 24.611572742462158 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.8428\n",
      "Epoch 100 Batch 100 Loss 0.8543\n",
      "Epoch 100 Batch 200 Loss 0.8660\n",
      "Epoch 100 Batch 300 Loss 0.8570\n",
      "Epoch 100 Loss 0.8621\n",
      "Time taken for 1 epoch 24.02376914024353 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.8655\n",
      "Epoch 101 Batch 100 Loss 0.8422\n",
      "Epoch 101 Batch 200 Loss 0.8399\n",
      "Epoch 101 Batch 300 Loss 0.8476\n",
      "Epoch 101 Loss 0.8669\n",
      "Time taken for 1 epoch 23.893197298049927 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.7863\n",
      "Epoch 102 Batch 100 Loss 0.8635\n",
      "Epoch 102 Batch 200 Loss 0.9116\n",
      "Epoch 102 Batch 300 Loss 0.8501\n",
      "Epoch 102 Loss 0.8644\n",
      "Time taken for 1 epoch 23.9593505859375 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.8257\n",
      "Epoch 103 Batch 100 Loss 0.8443\n",
      "Epoch 103 Batch 200 Loss 0.8760\n",
      "Epoch 103 Batch 300 Loss 0.8561\n",
      "Epoch 103 Loss 0.8646\n",
      "Time taken for 1 epoch 24.04351282119751 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.8461\n",
      "Epoch 104 Batch 100 Loss 0.8375\n",
      "Epoch 104 Batch 200 Loss 0.8655\n",
      "Epoch 104 Batch 300 Loss 0.8754\n",
      "Epoch 104 Loss 0.8889\n",
      "Time taken for 1 epoch 24.054259300231934 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.8342\n",
      "Epoch 105 Batch 100 Loss 0.8806\n",
      "Epoch 105 Batch 200 Loss 0.8637\n",
      "Epoch 105 Batch 300 Loss 0.8495\n",
      "Epoch 105 Loss 0.8772\n",
      "Time taken for 1 epoch 24.041067600250244 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.8246\n",
      "Epoch 106 Batch 100 Loss 0.8710\n",
      "Epoch 106 Batch 200 Loss 0.8419\n",
      "Epoch 106 Batch 300 Loss 0.8627\n",
      "Epoch 106 Loss 0.8712\n",
      "Time taken for 1 epoch 23.94851803779602 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.8361\n",
      "Epoch 107 Batch 100 Loss 0.8466\n",
      "Epoch 107 Batch 200 Loss 0.8518\n",
      "Epoch 107 Batch 300 Loss 0.8718\n",
      "Epoch 107 Loss 0.8479\n",
      "Time taken for 1 epoch 23.9587664604187 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.8289\n",
      "Epoch 108 Batch 100 Loss 0.8467\n",
      "Epoch 108 Batch 200 Loss 0.8628\n",
      "Epoch 108 Batch 300 Loss 0.8802\n",
      "Epoch 108 Loss 0.8320\n",
      "Time taken for 1 epoch 23.981553554534912 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.8425\n",
      "Epoch 109 Batch 100 Loss 0.8806\n",
      "Epoch 109 Batch 200 Loss 0.8412\n",
      "Epoch 109 Batch 300 Loss 0.8531\n",
      "Epoch 109 Loss 0.8794\n",
      "Time taken for 1 epoch 23.89494037628174 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.8155\n",
      "Epoch 110 Batch 100 Loss 0.8343\n",
      "Epoch 110 Batch 200 Loss 0.8238\n",
      "Epoch 110 Batch 300 Loss 0.8457\n",
      "Epoch 110 Loss 0.8616\n",
      "Time taken for 1 epoch 24.08424949645996 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.8140\n",
      "Epoch 111 Batch 100 Loss 0.8640\n",
      "Epoch 111 Batch 200 Loss 0.8601\n",
      "Epoch 111 Batch 300 Loss 0.8623\n",
      "Epoch 111 Loss 0.8694\n",
      "Time taken for 1 epoch 23.87120008468628 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.8347\n",
      "Epoch 112 Batch 100 Loss 0.8215\n",
      "Epoch 112 Batch 200 Loss 0.8394\n",
      "Epoch 112 Batch 300 Loss 0.8749\n",
      "Epoch 112 Loss 0.8746\n",
      "Time taken for 1 epoch 23.89226007461548 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.8272\n",
      "Epoch 113 Batch 100 Loss 0.8438\n",
      "Epoch 113 Batch 200 Loss 0.8804\n",
      "Epoch 113 Batch 300 Loss 0.8499\n",
      "Epoch 113 Loss 0.8798\n",
      "Time taken for 1 epoch 23.83267092704773 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.8266\n",
      "Epoch 114 Batch 100 Loss 0.8273\n",
      "Epoch 114 Batch 200 Loss 0.8336\n",
      "Epoch 114 Batch 300 Loss 0.8459\n",
      "Epoch 114 Loss 0.8488\n",
      "Time taken for 1 epoch 23.909188270568848 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.8123\n",
      "Epoch 115 Batch 100 Loss 0.8353\n",
      "Epoch 115 Batch 200 Loss 0.8397\n",
      "Epoch 115 Batch 300 Loss 0.8602\n",
      "Epoch 115 Loss 0.8807\n",
      "Time taken for 1 epoch 24.081889867782593 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.8226\n",
      "Epoch 116 Batch 100 Loss 0.8754\n",
      "Epoch 116 Batch 200 Loss 0.8545\n",
      "Epoch 116 Batch 300 Loss 0.8496\n",
      "Epoch 116 Loss 0.8665\n",
      "Time taken for 1 epoch 24.10165572166443 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.8170\n",
      "Epoch 117 Batch 100 Loss 0.8619\n",
      "Epoch 117 Batch 200 Loss 0.8398\n",
      "Epoch 117 Batch 300 Loss 0.8627\n",
      "Epoch 117 Loss 0.8640\n",
      "Time taken for 1 epoch 24.25841975212097 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.8441\n",
      "Epoch 118 Batch 100 Loss 0.8576\n",
      "Epoch 118 Batch 200 Loss 0.8382\n",
      "Epoch 118 Batch 300 Loss 0.8697\n",
      "Epoch 118 Loss 0.8293\n",
      "Time taken for 1 epoch 23.992841482162476 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.8174\n",
      "Epoch 119 Batch 100 Loss 0.8495\n",
      "Epoch 119 Batch 200 Loss 0.8580\n",
      "Epoch 119 Batch 300 Loss 0.8670\n",
      "Epoch 119 Loss 0.8507\n",
      "Time taken for 1 epoch 24.445050716400146 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.8218\n",
      "Epoch 120 Batch 100 Loss 0.8360\n",
      "Epoch 120 Batch 200 Loss 0.8467\n",
      "Epoch 120 Batch 300 Loss 0.8256\n",
      "Epoch 120 Loss 0.8529\n",
      "Time taken for 1 epoch 24.392780780792236 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.8389\n",
      "Epoch 121 Batch 100 Loss 0.8302\n",
      "Epoch 121 Batch 200 Loss 0.8408\n",
      "Epoch 121 Batch 300 Loss 0.8388\n",
      "Epoch 121 Loss 0.8480\n",
      "Time taken for 1 epoch 24.046202659606934 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.8332\n",
      "Epoch 122 Batch 100 Loss 0.8362\n",
      "Epoch 122 Batch 200 Loss 0.8813\n",
      "Epoch 122 Batch 300 Loss 0.8676\n",
      "Epoch 122 Loss 0.8707\n",
      "Time taken for 1 epoch 24.191635847091675 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.8279\n",
      "Epoch 123 Batch 100 Loss 0.8405\n",
      "Epoch 123 Batch 200 Loss 0.8540\n",
      "Epoch 123 Batch 300 Loss 0.8385\n",
      "Epoch 123 Loss 0.8475\n",
      "Time taken for 1 epoch 24.453344583511353 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.8372\n",
      "Epoch 124 Batch 100 Loss 0.8515\n",
      "Epoch 124 Batch 200 Loss 0.8413\n",
      "Epoch 124 Batch 300 Loss 0.8480\n",
      "Epoch 124 Loss 0.8734\n",
      "Time taken for 1 epoch 24.05837845802307 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.8245\n",
      "Epoch 125 Batch 100 Loss 0.8360\n",
      "Epoch 125 Batch 200 Loss 0.8301\n",
      "Epoch 125 Batch 300 Loss 0.8387\n",
      "Epoch 125 Loss 0.8541\n",
      "Time taken for 1 epoch 24.496917486190796 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.8215\n",
      "Epoch 126 Batch 100 Loss 0.8122\n",
      "Epoch 126 Batch 200 Loss 0.8413\n",
      "Epoch 126 Batch 300 Loss 0.8751\n",
      "Epoch 126 Loss 0.8418\n",
      "Time taken for 1 epoch 23.95917773246765 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.8029\n",
      "Epoch 127 Batch 100 Loss 0.8454\n",
      "Epoch 127 Batch 200 Loss 0.8416\n",
      "Epoch 127 Batch 300 Loss 0.8808\n",
      "Epoch 127 Loss 0.8781\n",
      "Time taken for 1 epoch 23.651850938796997 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.8166\n",
      "Epoch 128 Batch 100 Loss 0.8201\n",
      "Epoch 128 Batch 200 Loss 0.8712\n",
      "Epoch 128 Batch 300 Loss 0.8664\n",
      "Epoch 128 Loss 0.8631\n",
      "Time taken for 1 epoch 24.014241933822632 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.8256\n",
      "Epoch 129 Batch 100 Loss 0.8361\n",
      "Epoch 129 Batch 200 Loss 0.8425\n",
      "Epoch 129 Batch 300 Loss 0.8375\n",
      "Epoch 129 Loss 0.8567\n",
      "Time taken for 1 epoch 24.435497999191284 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.8289\n",
      "Epoch 130 Batch 100 Loss 0.8578\n",
      "Epoch 130 Batch 200 Loss 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 Batch 300 Loss 0.8263\n",
      "Epoch 130 Loss 0.8329\n",
      "Time taken for 1 epoch 24.293970108032227 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.8212\n",
      "Epoch 131 Batch 100 Loss 0.8263\n",
      "Epoch 131 Batch 200 Loss 0.8365\n",
      "Epoch 131 Batch 300 Loss 0.8384\n",
      "Epoch 131 Loss 0.8666\n",
      "Time taken for 1 epoch 25.46133804321289 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.8177\n",
      "Epoch 132 Batch 100 Loss 0.8725\n",
      "Epoch 132 Batch 200 Loss 0.8350\n",
      "Epoch 132 Batch 300 Loss 0.8401\n",
      "Epoch 132 Loss 0.8617\n",
      "Time taken for 1 epoch 25.064318656921387 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.7943\n",
      "Epoch 133 Batch 100 Loss 0.8228\n",
      "Epoch 133 Batch 200 Loss 0.8761\n",
      "Epoch 133 Batch 300 Loss 0.8300\n",
      "Epoch 133 Loss 0.8375\n",
      "Time taken for 1 epoch 24.281461477279663 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.8083\n",
      "Epoch 134 Batch 100 Loss 0.8615\n",
      "Epoch 134 Batch 200 Loss 0.8552\n",
      "Epoch 134 Batch 300 Loss 0.8722\n",
      "Epoch 134 Loss 0.8187\n",
      "Time taken for 1 epoch 23.725439071655273 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.8269\n",
      "Epoch 135 Batch 100 Loss 0.8582\n",
      "Epoch 135 Batch 200 Loss 0.8627\n",
      "Epoch 135 Batch 300 Loss 0.8183\n",
      "Epoch 135 Loss 0.8527\n",
      "Time taken for 1 epoch 24.204303979873657 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.8306\n",
      "Epoch 136 Batch 100 Loss 0.8334\n",
      "Epoch 136 Batch 200 Loss 0.8531\n",
      "Epoch 136 Batch 300 Loss 0.8550\n",
      "Epoch 136 Loss 0.8735\n",
      "Time taken for 1 epoch 23.60311770439148 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.7993\n",
      "Epoch 137 Batch 100 Loss 0.8325\n",
      "Epoch 137 Batch 200 Loss 0.8683\n",
      "Epoch 137 Batch 300 Loss 0.8503\n",
      "Epoch 137 Loss 0.8945\n",
      "Time taken for 1 epoch 23.82300853729248 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.7935\n",
      "Epoch 138 Batch 100 Loss 0.8698\n",
      "Epoch 138 Batch 200 Loss 0.8703\n",
      "Epoch 138 Batch 300 Loss 0.8471\n",
      "Epoch 138 Loss 0.8653\n",
      "Time taken for 1 epoch 24.08923578262329 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.8076\n",
      "Epoch 139 Batch 100 Loss 0.8308\n",
      "Epoch 139 Batch 200 Loss 0.8384\n",
      "Epoch 139 Batch 300 Loss 0.8557\n",
      "Epoch 139 Loss 0.8662\n",
      "Time taken for 1 epoch 23.92253828048706 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.8251\n",
      "Epoch 140 Batch 100 Loss 0.8244\n",
      "Epoch 140 Batch 200 Loss 0.8604\n",
      "Epoch 140 Batch 300 Loss 0.8448\n",
      "Epoch 140 Loss 0.8527\n",
      "Time taken for 1 epoch 23.83096742630005 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.8216\n",
      "Epoch 141 Batch 100 Loss 0.8622\n",
      "Epoch 141 Batch 200 Loss 0.8233\n",
      "Epoch 141 Batch 300 Loss 0.8399\n",
      "Epoch 141 Loss 0.8385\n",
      "Time taken for 1 epoch 23.87310481071472 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.8270\n",
      "Epoch 142 Batch 100 Loss 0.8349\n",
      "Epoch 142 Batch 200 Loss 0.8512\n",
      "Epoch 142 Batch 300 Loss 0.8416\n",
      "Epoch 142 Loss 0.8580\n",
      "Time taken for 1 epoch 24.182676315307617 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.7689\n",
      "Epoch 143 Batch 100 Loss 0.8512\n",
      "Epoch 143 Batch 200 Loss 0.8342\n",
      "Epoch 143 Batch 300 Loss 0.8377\n",
      "Epoch 143 Loss 0.8583\n",
      "Time taken for 1 epoch 23.8086256980896 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.8121\n",
      "Epoch 144 Batch 100 Loss 0.8222\n",
      "Epoch 144 Batch 200 Loss 0.8731\n",
      "Epoch 144 Batch 300 Loss 0.8420\n",
      "Epoch 144 Loss 0.8717\n",
      "Time taken for 1 epoch 23.996429681777954 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.8224\n",
      "Epoch 145 Batch 100 Loss 0.8068\n",
      "Epoch 145 Batch 200 Loss 0.8526\n",
      "Epoch 145 Batch 300 Loss 0.8277\n",
      "Epoch 145 Loss 0.8654\n",
      "Time taken for 1 epoch 24.104549169540405 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.8229\n",
      "Epoch 146 Batch 100 Loss 0.8333\n",
      "Epoch 146 Batch 200 Loss 0.8417\n",
      "Epoch 146 Batch 300 Loss 0.8563\n",
      "Epoch 146 Loss 0.8556\n",
      "Time taken for 1 epoch 24.047300338745117 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.8053\n",
      "Epoch 147 Batch 100 Loss 0.8382\n",
      "Epoch 147 Batch 200 Loss 0.8411\n",
      "Epoch 147 Batch 300 Loss 0.8591\n",
      "Epoch 147 Loss 0.8081\n",
      "Time taken for 1 epoch 23.99004101753235 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.8195\n",
      "Epoch 148 Batch 100 Loss 0.8384\n",
      "Epoch 148 Batch 200 Loss 0.8450\n",
      "Epoch 148 Batch 300 Loss 0.8294\n",
      "Epoch 148 Loss 0.8291\n",
      "Time taken for 1 epoch 24.145535707473755 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.8041\n",
      "Epoch 149 Batch 100 Loss 0.8405\n",
      "Epoch 149 Batch 200 Loss 0.8181\n",
      "Epoch 149 Batch 300 Loss 0.8412\n",
      "Epoch 149 Loss 0.8470\n",
      "Time taken for 1 epoch 24.15721607208252 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.8020\n",
      "Epoch 150 Batch 100 Loss 0.8831\n",
      "Epoch 150 Batch 200 Loss 0.8365\n",
      "Epoch 150 Batch 300 Loss 0.8371\n",
      "Epoch 150 Loss 0.8387\n",
      "Time taken for 1 epoch 24.292495012283325 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 150\n",
    "train_model(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     ckpt-24.data-00000-of-00001\r\n",
      "ckpt-10.data-00000-of-00001  ckpt-24.index\r\n",
      "ckpt-10.index\t\t     ckpt-25.data-00000-of-00001\r\n",
      "ckpt-11.data-00000-of-00001  ckpt-25.index\r\n",
      "ckpt-11.index\t\t     ckpt-26.data-00000-of-00001\r\n",
      "ckpt-12.data-00000-of-00001  ckpt-26.index\r\n",
      "ckpt-12.index\t\t     ckpt-27.data-00000-of-00001\r\n",
      "ckpt-13.data-00000-of-00001  ckpt-27.index\r\n",
      "ckpt-13.index\t\t     ckpt-28.data-00000-of-00001\r\n",
      "ckpt-14.data-00000-of-00001  ckpt-28.index\r\n",
      "ckpt-14.index\t\t     ckpt-29.data-00000-of-00001\r\n",
      "ckpt-15.data-00000-of-00001  ckpt-29.index\r\n",
      "ckpt-15.index\t\t     ckpt-2.data-00000-of-00001\r\n",
      "ckpt-16.data-00000-of-00001  ckpt-2.index\r\n",
      "ckpt-16.index\t\t     ckpt-30.data-00000-of-00001\r\n",
      "ckpt-17.data-00000-of-00001  ckpt-30.index\r\n",
      "ckpt-17.index\t\t     ckpt-3.data-00000-of-00001\r\n",
      "ckpt-18.data-00000-of-00001  ckpt-3.index\r\n",
      "ckpt-18.index\t\t     ckpt-4.data-00000-of-00001\r\n",
      "ckpt-19.data-00000-of-00001  ckpt-4.index\r\n",
      "ckpt-19.index\t\t     ckpt-5.data-00000-of-00001\r\n",
      "ckpt-1.data-00000-of-00001   ckpt-5.index\r\n",
      "ckpt-1.index\t\t     ckpt-6.data-00000-of-00001\r\n",
      "ckpt-20.data-00000-of-00001  ckpt-6.index\r\n",
      "ckpt-20.index\t\t     ckpt-7.data-00000-of-00001\r\n",
      "ckpt-21.data-00000-of-00001  ckpt-7.index\r\n",
      "ckpt-21.index\t\t     ckpt-8.data-00000-of-00001\r\n",
      "ckpt-22.data-00000-of-00001  ckpt-8.index\r\n",
      "ckpt-22.index\t\t     ckpt-9.data-00000-of-00001\r\n",
      "ckpt-23.data-00000-of-00001  ckpt-9.index\r\n",
      "ckpt-23.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt-30'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Generate text using the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_string='A', num_generate=1000):\n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\"\n",
      "\n",
      "Harry felt a great witch in front of him, and there was a soft thump and dangerous as his feet and set off at the sight of the first years. \n",
      "\n",
      "A sudden seats draw under his brain.\n",
      "\n",
      "He could see a silence so that his face was still staring at the egg in the dark and silver and glasses.\n",
      "\n",
      "\"Harry!\" she said. \"They were trying to stop us!\"\n",
      "\n",
      "Harry didn't say anything. He pulled out the stairs to the ground behind him.\n",
      "\n",
      "\"So that's what you mean, they were playing competing,\" said Lupin, still sitting\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(start_string='H', num_generate=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry is going to get there!\"\n",
      "\n",
      "\"I think I will be able to see the bushes,\" said Hagrid in a very fine at Krum's voice. He was looking at the first task filled the castle and started to show the houseelves in the shadows, and the sound of the solidement was so dark they could see through the darkness, the hand pointed out of the way to Harry to his feet. \"It was a minute \"\n",
      "\n",
      "\"The Dark Mark what they were talking about?\" said Ron, who was still sitting on the stone steps into the castle and the stairs t\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(start_string='Harry is', num_generate=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azkaban trees behind him.\n",
      "\n",
      "\"So what are you doing here?\" said Harry. \"I don't know what I see the third task.\"\n",
      "\n",
      "\"What?\" said Harry. \"There was the best school aloud. \n",
      "\n",
      "\"And what are you talking about?\" said Ron, who was still sitting on the stone steps into the castle and the stairs to the door, whose handle was sitting at the stands around the side of the staircase and pulled on the stone steps, to the floor. He was wearing a long sigh and started to do it. \n",
      "\n",
      "\"It's too late,\" said Harry. \"I would all \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(start_string='Azkaban', num_generate=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
